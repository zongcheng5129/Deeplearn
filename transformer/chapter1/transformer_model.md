 
## How do Transformers work?

![transformer model](image-1.png)

transformerçš„Encoderå’ŒDecoderéƒ¨åˆ†å¯ä»¥åˆ†å¼€ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥ä¸€èµ·ä½¿ç”¨
Each of these parts can be used independently, depending on the task:

1. Encoder-only models: Good for tasks that require understanding of the input, such as sentence classification and named entity recognition.

2. Decoder-only models: Good for generative tasks such as text generation.

3. Encoder-decoder models or sequence-to-sequence models: Good for generative tasks that require an input, such as translation or summarization.


### **ä¸‰ç§æ¨¡å¼çš„ä»£è¡¨ä½œå°±æ˜¯GPTï¼ŒBERTï¼ŒT5**:
```
1. BERT æ˜¯åšâ€œå®Œå½¢å¡«ç©ºâ€ï¼ˆçœ‹åˆ°å…¨å¥ï¼Œåªè¡¥å…¶ä¸­è¢«é®ç›–çš„è¯ï¼‰ã€‚Encoder-onlyï¼› masked language modeling; æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ç­‰**ç†è§£ç±»ä»»åŠ¡**

2. GPT æ˜¯åšâ€œç»­å†™å¡«ç©ºâ€ï¼ˆåªèƒ½çœ‹åˆ°å·¦è¾¹ï¼Œä¸æ–­å‘å³å†™æ–°è¯ï¼‰ã€‚Decoder-onlyï¼›
causal language modelingï¼šthe output depends on the past and present inputs, but not the future ones.
æ–‡æœ¬ç”Ÿæˆã€å¯¹è¯ã€ç»­å†™ã€ä»£ç ç”Ÿæˆç­‰**ç”Ÿæˆä»»åŠ¡**

3. T5 æ˜¯åšâ€œæ”¹å†™å¡«ç©ºâ€ï¼ˆè¾“å…¥ä¸€ä¸ªå¸¦å™ªå£°çš„å¥å­ï¼Œè¾“å‡ºå®Œæ•´çš„æ”¹æ­£åå¥å­ï¼‰ã€‚Encoder-Decoder;æ ¹æ®è¾“å…¥ç”Ÿæˆè¾“å‡ºçš„ä»»åŠ¡ï¼Œç›®æ ‡ï¼šå»å™ªè‡ªç¼–ç 
```

**attention layers** : will tell the model to pay specific attention to certain words in the sentence you passed it

### paper list:
```
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighte
GPT-1:Improving Language Understanding by Generative Pre-Training
GPT-2:Language Models are Unsupervised Multitask Learners
Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension
```

![detail model](image-2.png)

## How ğŸ¤— Transformers solve tasks

>**follow a similar pattern, different is :**
>1. the data is prepared
>2. what model architecture variant is used
>3. how the output is processed.

GPT2 uses byte pair encoding (BPE) to tokenize words and generate a token embedding.
what is byte pair encoding (BPE)

BERT uses WordPiece tokenization, user [SEP] (å¥å­ä¹‹é—´çš„è¿è´¯æ€§å’Œé€»è¾‘å…³ç³») and mask language modeling learning å•è¯çš„æ„æ€ï¼ˆé€šè¿‡æ©ç è¯­è¨€å»ºæ¨¡ï¼‰ï¼Œ 

identifying the most relevant words to predict the next token 